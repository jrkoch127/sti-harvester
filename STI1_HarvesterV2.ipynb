{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2761fe8",
   "metadata": {},
   "source": [
    "# STI/NTRS/PubSpace Harvester\n",
    "\n",
    "This notebook sends a query to STI/NTRS API for harvesting bibliographic records. Data returned are processed through the ADS Reference Service to weed out records already existing in ADS. The Excel workbook output can be reviewed and curated for ingest to ADS.\n",
    "___\n",
    "#### STI Harvest\n",
    "1. Set name for output file (date + category) and filepath\n",
    "2. Select API endpoint for querying (regular STI Collection vs Pubspace) and parameters\n",
    "3. Connect to STI API and pull/store data locally\n",
    "\n",
    "#### ADS Bibcode Matching\n",
    "4. Tranform STI results into ref strings\n",
    "5. Query the ADS Reference Service API with ref strings, return bibcode matches\n",
    "6. Output the records with bibcode results\n",
    "___\n",
    "NOTEBOOK OUTPUT: \n",
    "- Excel workbook; \"{name}_data.xlsx\" includes:\n",
    "   - bibliographic metadata retrieved from STI, curated for ADS\n",
    "   - index label: STI or CHORUS\n",
    "   - reference resolver data: refstring, score, and bibcode if matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50749732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import unicodedata\n",
    "from pyingest.serializers.classic import Tagged\n",
    "import re\n",
    "import csv\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# -- Set the year/name of output file\n",
    "name = \"2023\"\n",
    "# name = \"no_pub\"  # search for STI records that don't have a pubdate\n",
    "\n",
    "# -- Set local filepath\n",
    "filepath = \"/Users/sao/Documents/Python-Projects/NASA_STI/data/\"\n",
    "filename = name + \"_data.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50c6476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the filepath and file name to get the full path to the Excel file\n",
    "infile = os.path.join(filepath, filename)\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(infile):\n",
    "    # Create an empty DataFrame and save it as the Excel file\n",
    "    column_names = [\"ident\", \"authors\", \"affiliations\", \"title\", \"pubdate\", \"publication\", \"abstract\", \"properties\", \"collection\", \"keywords\", \"subjectcategory\", \"comment\", \"source\"]\n",
    "    data = pd.DataFrame(columns=column_names)\n",
    "    data.to_excel(infile, index=False)\n",
    "\n",
    "# Input query parameters\n",
    "gte=name+\"-01-01\"  # Greater than or equal to the first of the year\n",
    "lte=name+\"-12-31\"  # Less than or equal to the last of the year\n",
    "\n",
    "if name == \"no_pub\":\n",
    "    today = datetime.date.today()\n",
    "    year = today.year\n",
    "    month = today.month\n",
    "\n",
    "    # Generate the start and end dates for the current month\n",
    "    start_date = f\"{year}-{month:02d}-01\"  #YYYY-MM-01\n",
    "    end_date = f\"{year}-{month:02d}-{calendar.monthrange(year, month)[1]}\"  #YYYY-MM-31\n",
    "\n",
    "    # Set the parameters - search modified date of the current month\n",
    "    params = {\n",
    "        \"modified\": {\n",
    "            \"gte\": start_date,\n",
    "            \"lte\": end_date\n",
    "        },\n",
    "        \"page\": {\"size\": 100}\n",
    "    }\n",
    "\n",
    "elif name == \"2013\": # search for records with pubdate less than 2013-12-31\n",
    "    params = {\n",
    "         \"published\": {\n",
    "             \"lte\":lte\n",
    "         },\n",
    "        \"page\": {\"size\":100}\n",
    "    }\n",
    "else: # search for records with pubdate within year given\n",
    "    params = {\n",
    "         \"published\": {\n",
    "             \"gte\":gte,\n",
    "             \"lte\":lte\n",
    "         },\n",
    "        \"page\": {\"size\":100}\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d585b1e1",
   "metadata": {},
   "source": [
    "## NTRS API Query\n",
    "Retrieves all records from above parameters. \n",
    "Then grabs a handpicked set of desired metadata fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b9f30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Setup\n",
    "base_url = \"https://ntrs.nasa.gov/api\"\n",
    "path = \"/pubspace/search\"\n",
    "api_url = base_url + path\n",
    "MAX_RECORDS = 100\n",
    "\n",
    "def get_batch(api_url, params):\n",
    "    get_header = {'Accept': 'text/plain',\n",
    "                  'Content-type': 'application/json'}\n",
    "    buff = requests.post(api_url, headers=get_header, data=json.dumps(params)).json()\n",
    "    return buff\n",
    "\n",
    "def get_records(url, params):\n",
    "    records = []\n",
    "    \n",
    "    # Do the first query\n",
    "    try:\n",
    "        batch = get_batch(url, params)\n",
    "    except Exception as err:\n",
    "        raise Exception(\"Request to STI blew up: %s\" % err)\n",
    "        \n",
    "    # Count of total records\n",
    "    totrecs = batch['stats']['total']\n",
    " \n",
    "    # Store the first batch of records  \n",
    "    records += batch['results']\n",
    "    \n",
    "    # Print count of total records and pages\n",
    "    num_paginates = int(math.ceil((totrecs) / (1.0*MAX_RECORDS)))\n",
    "    print(\"> Total records: %d \\n > Total pages: %d\" % (totrecs, num_paginates))\n",
    "          \n",
    "    # Continue requests\n",
    "    offset = MAX_RECORDS\n",
    "    for i in range(num_paginates):\n",
    "        params['page']['from'] = offset\n",
    "        try:\n",
    "            batch = get_batch(url, params)\n",
    "        except Exception as err:\n",
    "            raise URLError(\"Request to STI blew up: %s\" % err)\n",
    "        records += batch['results']\n",
    "        offset += MAX_RECORDS\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e7818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run API Request\n",
    "from_sti = get_records(api_url, params)\n",
    "\n",
    "# Normalize json results to generate DataFrame\n",
    "dt = pd.json_normalize(from_sti, meta=['title'])\n",
    "\n",
    "# List of desired fields\n",
    "desired_fields = [\n",
    "        \"id\",\n",
    "        \"subjectCategories\",\n",
    "        \"fundingNumbers\",\n",
    "        \"authorAffiliations\",\n",
    "        \"title\",\n",
    "        \"stiType\",\n",
    "        \"abstract\",\n",
    "        \"publications\",\n",
    "        \"center.code\",\n",
    "        \"keywords\",\n",
    "        \"sourceIdentifiers\",\n",
    "        \"meetings\",\n",
    "        \"downloads\",\n",
    "        \"index\"\n",
    "    ]\n",
    "\n",
    "# Grab desired fields if present in dt\n",
    "df = dt[[col for col in desired_fields if col in dt.columns]]\n",
    "\n",
    "# Drop rows where Document ID is null\n",
    "df = df.dropna(subset=['id'])\n",
    "\n",
    "# Drop duplicates by Document ID\n",
    "df = df.drop_duplicates(subset=['id'], keep='last')\n",
    "\n",
    "print(\"Harvested %d records\"%(len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ed8e32",
   "metadata": {},
   "source": [
    "## Metadata Wrangling\n",
    "Transforms exisiting STI metadata into ADS desired metadata:\n",
    "Title, Abstract, Keywords, Collection, Subject Category (Earth Science), Authors and their affiliations, STI id, Funding numbers, NASA Center of origin, NTRS index source (STI v. CHORUS), publication info, links/dois."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f942f6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TITLE/T\n",
    "lsT = [t if t else '' for t in df['title'] if 'abstract' in df.columns]\n",
    "\n",
    "# ABSTRACT/B\n",
    "lsB = [b if b else '' for b in df['abstract'] if 'abstract' in df.columns]\n",
    "\n",
    "# KEYWORDS/K - Put STI Subject Category into %K\n",
    "lsK = []\n",
    "if 'keywords' in df.columns and 'subjectCategories' in df.columns:\n",
    "    for s, k, in zip(df['subjectCategories'], df['keywords']):\n",
    "        keywords = ''\n",
    "        if isinstance(s, list):\n",
    "            keywords += ', '.join(s)\n",
    "        if isinstance(k, list):\n",
    "            keywords += ', ' + ', '.join(k)\n",
    "        lsK.append(keywords.lstrip(', '))\n",
    "else:\n",
    "    lsK = [''] * len(df)\n",
    "\n",
    "# COLLECTION/W & SUBJECT CATEGORY/Q - Assign ADS %W and %Q based on STI Subject Categories\n",
    "AST_cats = [\"Astronomy\", \"Astrophysics\"]\n",
    "PHY_cats = [\"Physics (General)\", \"Acoustics\", \"Atomic And Molecular Physics\",\n",
    "    \"Nuclear Physics\", \"Optics\", \"Plasma Physics\", \"Solid-State Physics\",\n",
    "    \"Physics Of Elementary Particles And Fields\", \"Astronautics (General)\",\n",
    "    \"Astrodynamics\", \"Ground Support Systems And Facilities (Space)\",\n",
    "    \"Launch Vehicles And Launch Operations\", \"Space Transportation And Safety\",\n",
    "    \"Space Communications, Spacecraft Communications, Command And Tracking\",\n",
    "    \"Spacecraft Design, Testing And Performance\", \"Spacecraft Instrumentation And Astrionics\",\n",
    "    \"Spacecraft Propulsion And Power\", \"Engineering (General)\",\n",
    "    \"Communications And Radar\", \"Electronics And Electrical Engineering\",\n",
    "    \"Fluid Mechanics And Thermodynamics\", \"Instrumentation And Photography\",\n",
    "    \"Lasers And Masers\", \"Mechanical Engineering\",\n",
    "    \"Quality Assurance And Reliability\", \"Structural Mechanics\"]\n",
    "ES_cats = [\"Geosciences (General)\", \"Earth Resources And Remote Sensing\",\n",
    "    \"Energy Production And Conversion\", \"Environment Pollution\",\n",
    "    \"Geophysics\", \"Meteorology And Climatology\", \"Oceanography\"]\n",
    "# PS_cats = [\"Space Sciences (General)\", \"Lunar and Planetary Science and Exploration\", \"Exobiology\"]\n",
    "# BPS_cats = [\"Life Sciences (General)\", \"Aerospace Medicine\", \"Behavioral Sciences\", \"Man/System Technology and Life Support\"]\n",
    "# HP_cats = [\"Solar Physics\", \"Space Radiation\"]\n",
    "\n",
    "lsW = []  # Collection\n",
    "lsQ = []  # Subject category (Earth Sciencce)\n",
    "if 'subjectCategories' in df.columns:\n",
    "    for subjects in df['subjectCategories']:\n",
    "        if isinstance(subjects, float) and np.isnan(subjects):\n",
    "            # Skip iteration if the value is NaN\n",
    "            lsW.append('GEN')\n",
    "            lsQ.append('')\n",
    "            continue\n",
    "        if any(subject in AST_cats for subject in subjects):\n",
    "            lsW.append('AST')\n",
    "            lsQ.append('')\n",
    "        elif any(subject in ES_cats for subject in subjects):\n",
    "            lsW.append('GEN')\n",
    "            lsQ.append('Earth Science')\n",
    "        elif any(subject in PHY_cats for subject in subjects):\n",
    "            lsW.append('PHY')\n",
    "            lsQ.append('')\n",
    "        else:\n",
    "            lsW.append('GEN')\n",
    "            lsQ.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b237ab3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTHORS/A & AFFILIATIONS/F\n",
    "\n",
    "# Function to reformat author names if not already in format \"Last, First\"\n",
    "    # Also appends periods '.' to initials if necessary\n",
    "def reformat_author(author_name):\n",
    "   \n",
    "    # If comma in author name, assume it's in \"Last, First\" format already; no need to reformat\n",
    "    if ', ' in author_name:\n",
    "        name_parts = author_name.split(', ')\n",
    "        last_name = name_parts[0]\n",
    "        \n",
    "        if len(name_parts) > 1:\n",
    "            first_middle_parts = name_parts[1].split()\n",
    "            first_name = first_middle_parts[0] if first_middle_parts else ''\n",
    "            middle_name = ' '.join(first_middle_parts[1:]) if len(first_middle_parts) > 1 else ''\n",
    "        else:\n",
    "            first_name = ''\n",
    "            middle_name = ''\n",
    "\n",
    "        # Check and add period to first name if necessary\n",
    "        if len(first_name) == 1 and not first_name.endswith('.'):\n",
    "            first_name += '.'\n",
    "\n",
    "        # Check and add period to middle name if necessary\n",
    "        initials = middle_name.split()\n",
    "        for i in range(len(initials)):\n",
    "            if len(initials[i]) == 1 and not initials[i].endswith('.'):\n",
    "                initials[i] += '.'\n",
    "        middle_name = ' '.join(initials)\n",
    "\n",
    "        formatted_name = \"{}, {}\".format(last_name, first_name)\n",
    "        if middle_name:\n",
    "            formatted_name += \" {}\".format(middle_name)\n",
    "\n",
    "        return formatted_name\n",
    "    \n",
    "    # Reformat if there's no comma in author_name\n",
    "    else:\n",
    "        name_list = author_name.split()\n",
    "        if len(name_list) > 0:\n",
    "            first_name = name_list[0]\n",
    "            last_name = \"\"\n",
    "            middle_name = \"\"\n",
    "            suffix = \"\"\n",
    "\n",
    "            # Check for suffixes\n",
    "            suffixes = [\"Jr.\", \"Sr.\", \"II\", \"III\"]\n",
    "            if len(name_list) > 1:\n",
    "                last_word = name_list[-1]\n",
    "                if last_word in suffixes:\n",
    "                    suffix = last_word\n",
    "                    name_list = name_list[:-1]\n",
    "\n",
    "                # Format name parts\n",
    "                if len(name_list) > 1:\n",
    "                    last_name = name_list[-1]\n",
    "                    middle_name = \" \".join(name_list[1:-1])\n",
    "                else:\n",
    "                    last_name = name_list[0]\n",
    "\n",
    "                # Check and add period to first name if necessary\n",
    "                if len(first_name) == 1 and not first_name.endswith('.'):\n",
    "                    first_name += '.'\n",
    "\n",
    "                # Check and add period to middle name if necessary\n",
    "                if len(middle_name) == 1 and not middle_name.endswith('.'):\n",
    "                    middle_name += '.'\n",
    "\n",
    "            # Construct formatted name\n",
    "            formatted_name = \"{}, {}\".format(last_name, first_name)\n",
    "            if middle_name:\n",
    "                formatted_name += \" {}\".format(middle_name)\n",
    "            if suffix:\n",
    "                formatted_name += \", {}\".format(suffix)\n",
    "            return formatted_name\n",
    "\n",
    "\n",
    "# Grab Author/Affiliation Metadata\n",
    "lsA = []  # Authors List\n",
    "lsF = []  # Affiliations List\n",
    "\n",
    "if 'authorAffiliations' in df.columns:\n",
    "    for authors in df['authorAffiliations']:\n",
    "        row_authors = []\n",
    "        row_affils = []\n",
    "\n",
    "        if isinstance(authors, list):\n",
    "            for entry in authors:\n",
    "                author = entry['meta']['author'].get('name', '')  # Concat author names\n",
    "                reformatted = reformat_author(author)\n",
    "                if reformatted:  # Only append non-empty author names\n",
    "                    row_authors.append(reformatted)\n",
    "\n",
    "                organization = entry['meta'].get('organization', {})  # Concat aff names\n",
    "                aff_name = organization.get('name', '')\n",
    "                aff_loc = organization.get('location', '')  # Concat aff locations\n",
    "                affil = ', '.join(filter(None, [aff_name, aff_loc]))\n",
    "\n",
    "                orcid = entry['meta']['author'].get('orcidId', '')  # Concat orcids\n",
    "                if orcid:\n",
    "                    affil += ' <ID system=\"ORCID\">{}</ID>'.format(orcid)\n",
    "\n",
    "                if affil:  # Only append non-empty affiliations\n",
    "                    row_affils.append(affil)\n",
    "                    \n",
    "            lsA.append(row_authors)\n",
    "            lsF.append(row_affils)\n",
    "\n",
    "        else:\n",
    "            lsA.append('')\n",
    "            lsF.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da566c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STI Stuff\n",
    "sources = [\"CHORUS\" if \"chorus\" in i else \"STI\" for i in df['index']]\n",
    "STI_ids = [d if d else '' for d in df['id']]\n",
    "lsX = [f\"NASA Center: {c}\" if pd.notna(c) else '' for c in df['center.code'] if 'center.code' in df.columns]\n",
    "    \n",
    "# Funding Numbers\n",
    "fundingNums = []\n",
    "if 'fundingNumbers' in df.columns:\n",
    "    for numbers in df['fundingNumbers']:\n",
    "        if isinstance(numbers, list):\n",
    "            for entry in numbers:\n",
    "                if isinstance(entry, dict) and 'type' in entry and 'number' in entry:\n",
    "                    fundType = entry['type']\n",
    "                    fundNum = entry['number']\n",
    "                    fundingNums.append('{}: {}'.format(fundType, fundNum))\n",
    "                else:\n",
    "                    fundingNums.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee1884b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PUBDATE/D, PUBLICATION/J, LINK PROPERTIES/I \n",
    "\n",
    "# Initialize metadata lists\n",
    "pubs_ls = [] # Pubnames\n",
    "vols_ls = [] # Volumes\n",
    "dois_ls = [] # DOIs\n",
    "lsD = []  # Pubdates\n",
    "lsJ = []  # Journal/Pub\n",
    "lsI = []  # Properties/Links\n",
    "\n",
    "for pubs, docID, idents, dwnlds, funds  in zip(df['publications'], STI_ids, df['sourceIdentifiers'], df['downloads'], df['fundingNumbers']):\n",
    "    j = ''\n",
    "    links = ''\n",
    "    pubnames = ''\n",
    "    volumes = ''\n",
    "    dois = ''\n",
    "    \n",
    "    for entry in pubs:\n",
    "        pubdate = entry.get('publicationDate', '')[:10]\n",
    "        publication_name = entry.get('publicationName', '')\n",
    "        vol = entry.get('volume', '')\n",
    "        issue = entry.get('issue', '')\n",
    "        publisher = entry.get('publisher', '')\n",
    "        issn = entry.get('issn', '')\n",
    "        eissn = entry.get('eissn', '')\n",
    "        isbn = entry.get('isbn', '')\n",
    "        eisbn = entry.get('eisbn', '')\n",
    "        url = entry.get('url', '')\n",
    "        doi = entry.get('doi', '')\n",
    "            \n",
    "        if publication_name:\n",
    "            j += publication_name.rstrip(\" \")\n",
    "            pubnames += publication_name.rstrip(\" \")\n",
    "        if vol:\n",
    "            j += ', Vol. {}'.format(vol)\n",
    "            volumes += vol\n",
    "        if issue:\n",
    "            j += ', Issue {}'.format(issue)\n",
    "        if publisher:\n",
    "            j += ', Published by {}'.format(publisher.lstrip(\" \"))\n",
    "        if pubdate:\n",
    "            j += ', {}'.format(pubdate[:4].rstrip(\" \"))\n",
    "        if issn:\n",
    "            j += ', ISSN: {}'.format(issn)\n",
    "        if eissn:\n",
    "            j += ', eISSN: {}'.format(eissn)\n",
    "        if isbn:\n",
    "            j += ', ISBN: {}'.format(isbn)\n",
    "        if eisbn:\n",
    "            j += ', eISBN: {}'.format(isbn)\n",
    "        j = j.lstrip(', ')\n",
    "\n",
    "        # Concat link info from publications field\n",
    "        doi = doi.replace('https://', '').replace('doi:', '').replace('doi.org/', '').replace('DOI:', '').lstrip(\" \")\n",
    "        if url and doi:\n",
    "            links += 'ELECTR: {}; DOI: {}'.format(url, doi)\n",
    "            dois += doi\n",
    "        elif url:\n",
    "            links += 'ELECTR: {}'.format(url)\n",
    "        elif doi:\n",
    "            links += 'DOI: {}'.format(doi)\n",
    "            dois += doi\n",
    "    \n",
    "    links += '; ELECTR: https://ntrs.nasa.gov/citations/{}'.format(docID)  # STI LINK\n",
    "        \n",
    "    if 'meetings' in df.columns:\n",
    "        meets = df['meetings']\n",
    "        if isinstance(meets, list):         # Concat meeting info\n",
    "            for entry in meets:\n",
    "                if isinstance(entry, dict) and 'name' in entry and entry.get('name') != '':\n",
    "                    if 'location' in entry and entry.get('location') != '':\n",
    "                        meet = entry['name']\n",
    "                        meet_loc = entry['location']\n",
    "                        j += \"; {}, {}\".format(meet, meet_loc)\n",
    "                    else:\n",
    "                        j += \"; {}.\".format(meet)\n",
    "    \n",
    "    if isinstance(idents, list):       # Concat additional links from sourceIdentifiers field\n",
    "        for entry in idents:\n",
    "            if isinstance(entry, dict):\n",
    "                if entry.get('type') == 'URL' and 'number' in entry:\n",
    "                    if 'arXiv' in entry['number'] or 'arxiv.org' in entry['number']:\n",
    "                        arxiv = entry['number'].replace('arXiv:','')\n",
    "                        links += '; ARXIV: {}'.format(arxiv)\n",
    "                    else:\n",
    "                        url = entry['number']\n",
    "                        links += '; ELECTR: {}'.format(url)\n",
    "                if entry.get('type') == 'DOI' and 'number' in entry:\n",
    "                    doi = entry['number'].replace('doi:', '')\n",
    "                    links += '; DOI: {}'.format(doi)\n",
    "                    dois += doi\n",
    "                    \n",
    "    if isinstance(dwnlds, list):       # FULL TEXT LINKS\n",
    "        for entry in dwnlds:\n",
    "            if isinstance(entry, dict) and 'links' in entry:\n",
    "                if 'pdf' in entry['links']:\n",
    "                    dwnld = entry['links']['pdf']\n",
    "                    pdf = 'https://ntrs.nasa.gov{}'.format(dwnld)\n",
    "                    links += '; PDF: {}'.format(pdf)\n",
    "                if 'fulltext' in entry['links']:\n",
    "                    dwnld = entry['links']['fulltext']\n",
    "                    fulltext = 'https://ntrs.nasa.gov{}'.format(dwnld)\n",
    "                    links += '; RAW: {}'.format(fulltext)\n",
    "            \n",
    "    if isinstance(funds, list):\n",
    "        for entry in funds:\n",
    "            if isinstance(entry, dict) and 'type' in entry and 'number' in entry:\n",
    "                if entry['type'] == 'CONTRACT_GRANT':\n",
    "                    fundNum = entry['number']\n",
    "                    links += '; GRANT: {}'.format(fundNum)\n",
    "\n",
    "    # Append metadata to lists for refstrings and curation/ingest\n",
    "    pubs_ls.append(pubnames)\n",
    "    vols_ls.append(volumes)\n",
    "    dois_ls.append(dois)\n",
    "    lsD.append(pubdate)\n",
    "    lsJ.append(j.lstrip('; '))\n",
    "    lsI.append(links.lstrip('; '))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06304983",
   "metadata": {},
   "source": [
    "# # Record creation and resolver service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb61d5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Excel file of records\n",
    "dt = pd.read_excel(filepath + filename)\n",
    "\n",
    "# Create a set of normalized identifiers from the Excel file; with whitespace stripped and converted to lowercase\n",
    "identifiers_ls = [str(ident).strip() for ident in dt['ident']]\n",
    "\n",
    "# Prepare a set of existing identifiers for quick lookup\n",
    "existing_identifiers = set(identifiers_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59180dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_list = []  # reference strings to query ADS Reference Resolver Service\n",
    "records = []   # records meetadata\n",
    "\n",
    "# Iterate through the data and generate refstrings and create records for new records\n",
    "for ident, A, D, T, pub, vol, doi, F, J, B, I, W, K, Q, X, source in zip(STI_ids, lsA, lsD, lsT, pubs_ls, vols_ls, dois_ls, lsF, lsJ, lsB, lsI, lsW, lsK, lsQ, lsX, sources):\n",
    "    normalized_ident = str(ident).strip()  # Normalize the identifier\n",
    "\n",
    "    # Check if the identifier is not in the existing set (new record)\n",
    "    if normalized_ident not in existing_identifiers:\n",
    "    \n",
    "        if isinstance(A, list):  # Grab just the first 10 authors\n",
    "            auth = '; '.join(A[:10])\n",
    "        \n",
    "        if D is not None:\n",
    "            D = D[:4]  # Year = first 4 digits of pubdate\n",
    "\n",
    "        if all(item is not None for item in [auth, D, T, doi]):   # author, year, title, and doi\n",
    "            ref = {\n",
    "                \"refstr\": \"%s, %s, %s, doi: %s\" % (auth, D, T, doi),\n",
    "                \"authors\": auth,\n",
    "                \"year\": D,\n",
    "                \"title\": T,\n",
    "                \"doi\": doi\n",
    "            }\n",
    "        elif all(item is not None for item in [auth, D, T, pub, vol]):  # author, year, title, journal, and volume\n",
    "            ref = {\n",
    "                \"refstr\": \"%s, %s, %s, %s %s\" % (auth, D, T, pub, vol),\n",
    "                \"authors\": auth,\n",
    "                \"year\": D,\n",
    "                \"title\": T,\n",
    "                \"journal\": \"%s %s\" % (pub, vol)\n",
    "            }\n",
    "        elif all(item is not None for item in [auth, D, T, pub]):  # author, year, title, and journal\n",
    "            ref = {\n",
    "                \"refstr\": \"%s, %s, %s, %s\" % (auth, D, T, pub),\n",
    "                \"authors\": auth,\n",
    "                \"year\": D,\n",
    "                \"title\": T,\n",
    "                \"journal\": pub\n",
    "            }\n",
    "        elif all(item is not None for item in [auth, D, T]):  # author, year, and title\n",
    "            ref = {\n",
    "                \"refstr\": \"%s, %s, %s\" % (auth, D, T),\n",
    "                \"authors\": auth,\n",
    "                \"year\": D,\n",
    "                \"title\": T\n",
    "            }\n",
    "        elif all(item is not None for item in [D, T]):  # year and title\n",
    "            ref = {\n",
    "                \"refstr\": \"%s, %s\" % (D, T),\n",
    "                \"year\": D,\n",
    "                \"title\": T\n",
    "            }\n",
    "        elif all(item is not None for item in [auth, T, doi]):   # author, title, and doi\n",
    "            ref = {\n",
    "                \"refstr\": \"%s, %s, doi: %s\" % (auth, T, doi),\n",
    "                \"authors\": auth,\n",
    "                \"title\": T,\n",
    "                \"doi\": doi\n",
    "            }\n",
    "        elif all(item is not None for item in [auth, T, pub, vol]):  # author, title, journal, and volume\n",
    "            ref = {\n",
    "                \"refstr\": \"%s, %s, %s %s\" % (auth, T, pub, vol),\n",
    "                \"authors\": auth,\n",
    "                \"title\": T,\n",
    "                \"journal\": \"%s %s\" % (pub, vol)\n",
    "            }\n",
    "        elif all(item is not None for item in [auth, T, pub]):  # author, title, and journal\n",
    "            ref = {\n",
    "                \"refstr\": \"%s, %s, %s\" % (auth, T, pub),\n",
    "                \"authors\": auth,\n",
    "                \"title\": T,\n",
    "                \"journal\": pub\n",
    "            }   \n",
    "        elif all(item is not None for item in [auth, T]):  # author and title\n",
    "            ref = {\n",
    "                \"refstr\": \"%s, %s\" % (auth, T),\n",
    "                \"authors\": auth,\n",
    "                \"title\": T\n",
    "            }\n",
    "        else:\n",
    "            ref = {\"refstr\":\"\"}\n",
    "\n",
    "        ref_string = json.dumps(ref, ensure_ascii=False)\n",
    "        ref_list.append(ref_string)\n",
    "\n",
    "        record = {\n",
    "            \"ident\": normalized_ident,\n",
    "            \"authors\": '; '.join([a for a in A if a is not None]),\n",
    "            \"affiliations\": '; '.join([f for f in F if f is not None]),\n",
    "            \"title\": T,\n",
    "            \"pubdate\": D,\n",
    "            \"publication\": J,\n",
    "            \"abstract\": B,\n",
    "            \"properties\": I,\n",
    "            \"collection\": W,\n",
    "            \"keywords\": K,\n",
    "            \"subjectcategory\": Q,\n",
    "            \"comment\": X,\n",
    "            \"source\": source,\n",
    "            \"refstring\": ref_string\n",
    "        }\n",
    "        records.append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c52d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Reference Resolver Service Setup\n",
    "\n",
    "# ADS Prod API Token\n",
    "token = 'pHazHxvHjPVPAcotvj7DIijROZXUjG5vXa2OaCQO'\n",
    "domain = 'https://api.adsabs.harvard.edu/v1/'\n",
    "\n",
    "# Reference Service API request, querying my 'references' list\n",
    "def resolve(references):\n",
    "    payload = {'parsed_reference': references}\n",
    "    response = requests.post(\n",
    "        url = domain + 'reference/xml',\n",
    "        headers = {'Authorization': 'Bearer ' + token,\n",
    "                 'Content-Type': 'application/json',\n",
    "                 'Accept':'application/json'},\n",
    "        data = json.dumps(payload))\n",
    "    if response.status_code == 200:\n",
    "        return json.loads(response.content)['resolved'], 200\n",
    "    else:\n",
    "        print('From reference status_code is ', response.status_code)\n",
    "    return None, response.status_code\n",
    "\n",
    "# -- Run Reference Resolver Service\n",
    "references = ref_list\n",
    "references = [ref.replace(\"\\n\",\" \") for ref in references]\n",
    "references = [json.loads(ref) for ref in references]\n",
    "\n",
    "# Resolve my references, results in 'total results' list\n",
    "total_results = []\n",
    "print('Querying %d references with the Reference Service ...'%len(references))\n",
    "for i in range(0, len(references), 16):\n",
    "    results, status = resolve(references[i:i+16])\n",
    "    if status == 200:\n",
    "        if results:\n",
    "            total_results += results\n",
    "#     else:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9b1746",
   "metadata": {},
   "source": [
    "## Output Results/Update Spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a334df25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the records with the reference results and output the data\n",
    "merged_data = []\n",
    "\n",
    "if len(records)>0 and len(records)==len(total_results):\n",
    "    for record, result in zip(records, total_results):\n",
    "        merged_entry = {\n",
    "            **record,  # Include all fields from the 'record' dictionary\n",
    "            'refstring': result['refstring'],  # Include 'refstring' from the 'result' dictionary\n",
    "            'score': result['score'],  # Include 'score' from the 'result' dictionary\n",
    "            'bibcode': result['bibcode'],  # Include 'bibcode' from the 'result' dictionary\n",
    "    #             **result,\n",
    "        }\n",
    "        merged_data.append(merged_entry)\n",
    "    print('merged results')\n",
    "\n",
    "    # Read the Excel file into a DataFrame\n",
    "    existing_data = pd.read_excel(filepath + filename)\n",
    "\n",
    "    # Create a DataFrame from the merged data, taking only records w/o pubdate if for no_pubs\n",
    "    merged_data_df = pd.DataFrame(merged_data)\n",
    "    if name == \"no_pub\":\n",
    "        merged_data_df = merged_data_df[(pd.isnull(merged_data_df['pubdate']))]\n",
    "\n",
    "    # Count non matched items in the merged data\n",
    "    not_matched_count = (merged_data_df['bibcode'] == '...................').sum()\n",
    "    matched_count = len(merged_data_df) - not_matched_count\n",
    "\n",
    "    # Concatenate the existing data and merged data vertically\n",
    "    combined_data = pd.concat([existing_data, merged_data_df], ignore_index=True)\n",
    "\n",
    "    # Write the combined data back to the Excel file\n",
    "    combined_data.to_excel(filepath + filename, index=False)\n",
    "\n",
    "    print(f'Saved data to {filename}: \\n\\\n",
    "          {len(merged_data)} new records \\n\\\n",
    "            > {matched_count} new matched \\n\\\n",
    "            > {not_matched_count} new not matched')\n",
    "elif len(records)==0:\n",
    "    print('No records to add')\n",
    "else:\n",
    "    print(f'records: {len(records)}')\n",
    "    print(f'results: {len(total_results)}')\n",
    "\n",
    "    # Output the data/results from a single harvest to xlsx\n",
    "    df1 = pd.DataFrame(records)\n",
    "    df2 = pd.DataFrame(total_results)\n",
    "\n",
    "    outfile = name + \"_review.xlsx\"\n",
    "    with pd.ExcelWriter(outfile) as writer:\n",
    "        df1.to_excel(writer, sheet_name='records', index=False)\n",
    "        df2.to_excel(writer, sheet_name='results', index=False)\n",
    "    print(f'Saved data to {outfile}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818c0c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add identifiers to 'all_data' set\n",
    "all_data = \"STI_all_data.xlsx\"\n",
    "# filename = \"null_pub.xlsx\"\n",
    "\n",
    "# Load the first Excel sheet\n",
    "sheet1 = pd.read_excel(filepath + filename)\n",
    "\n",
    "# Load the second Excel sheet\n",
    "sheet2 = pd.read_excel(filepath + all_data)\n",
    "\n",
    "# Convert idents from each sheet to a list\n",
    "idents_to_append = sheet1['ident'].tolist()\n",
    "all_list = sheet2['ident'].tolist()\n",
    "\n",
    "# Convert the all_list to a set to keep only unique values\n",
    "all_set = set(all_list)\n",
    "\n",
    "# Add only unique idents to the all_set\n",
    "for ident in idents_to_append:\n",
    "    if ident not in all_set:\n",
    "        all_list.append(ident)\n",
    "        all_set.add(ident)\n",
    "\n",
    "# Write the new idents list back to all_data\n",
    "df = pd.DataFrame({'ident': all_list})\n",
    "df.to_excel(filepath + all_data, index=False)\n",
    "print(f\"Number of all records: {len(df)}\")\n",
    "\n",
    "## Stats\n",
    "# Count the number of rows with 'STI' and 'CHORUS' in the 'source' column\n",
    "sti_count = len(sheet1[sheet1['source'] == 'STI'])\n",
    "chorus_count = len(sheet1[sheet1['source'] == 'CHORUS'])\n",
    "\n",
    "# Count the number of rows with '...................' in the 'bibcode' column\n",
    "null_bibcode_count = len(sheet1[sheet1['bibcode'] == '...................'])\n",
    "\n",
    "# Count the number of rows with a different value (not null) in the 'bibcode' column\n",
    "bibcode_count = len(sheet1[(sheet1['bibcode'].notnull()) & (sheet1['bibcode'] != '...................')])\n",
    "\n",
    "# Print the counts\n",
    "print(f'Stats for {name}')\n",
    "print(f'Number of rows with \"STI\": {sti_count}')\n",
    "print(f'Number of rows with \"CHORUS\": {chorus_count}')\n",
    "print(f'Number of rows with no bibcode: {null_bibcode_count}')\n",
    "print(f'Number of rows with a bibcode: {bibcode_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bee836e",
   "metadata": {},
   "source": [
    "## Additional Resolver \n",
    "Take this additional step to see if any previously unmatched items can be matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a462149c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Excel file into a DataFrame\n",
    "# filename = \"2023_data.xlsx\"\n",
    "master_data = pd.read_excel(filepath + filename)\n",
    "\n",
    "# Get rows with a bibcode value\n",
    "rows_with_bibcode = master_data[master_data['bibcode'] != '...................']\n",
    "count_with_bibcode = len(rows_with_bibcode)\n",
    "print(f\"Rows with a bibcode: {count_with_bibcode}\")\n",
    "\n",
    "# Get rows with no bibcode\n",
    "rows_with_no_bibcode = master_data[master_data['bibcode'] == '...................']\n",
    "count_no_bibcode = len(rows_with_no_bibcode)\n",
    "print(f\"Rows with no bibcode: {count_no_bibcode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7315513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list for references\n",
    "ref_list2 = []\n",
    "\n",
    "# Iterate through rows with no bibcode\n",
    "for index, row in rows_with_no_bibcode.iterrows():\n",
    "    D = row[\"pubdate\"]\n",
    "    T = row[\"title\"]\n",
    "    A = row[\"authors\"]\n",
    "    properties_str = str(row[\"properties\"])  # Convert to string\n",
    "    doi_match = re.search(r'(10\\.\\d+\\/\\S+)', properties_str)\n",
    "    if doi_match:\n",
    "        doi = doi_match.group(0).rstrip(\";\")\n",
    "    else:\n",
    "        doi = None\n",
    "    \n",
    "    if isinstance(row[\"publication\"], str):\n",
    "        J = row[\"publication\"].split(\", \")\n",
    "    else:\n",
    "        J = []  \n",
    "    \n",
    "    if all(item is not None for item in [A, T, D, doi]) and not pd.isna(doi):\n",
    "        ref = {\n",
    "            \"refstr\": f\"{A}, {T}, {str(D)}, {doi}\",\n",
    "            \"authors\": A,\n",
    "            \"title\": T,\n",
    "            \"year\": str(D),\n",
    "            \"doi\": doi\n",
    "        }\n",
    "    elif all(item is not None for item in [A, T, D]):\n",
    "        ref = {\n",
    "            \"refstr\": f\"{A}, {T}, {str(D)}\",\n",
    "            \"authors\": A,\n",
    "            \"title\": T,\n",
    "            \"year\": str(D)\n",
    "        }\n",
    "    elif doi is not None and not pd.isna(doi):\n",
    "        ref = {\n",
    "            \"refstr\": f\"{doi}\",\n",
    "            \"doi\": doi\n",
    "        }\n",
    "    else:\n",
    "        ref = {\"refstr\":\"\"}\n",
    "    \n",
    "    ref_string = json.dumps(ref, ensure_ascii=False)\n",
    "#     print(ref_string, '\\n')\n",
    "    ref_list2.append(ref_string)\n",
    "\n",
    "# Reference Service API request, querying my 'references' list\n",
    "# ADS Prod API Token\n",
    "token = 'pHazHxvHjPVPAcotvj7DIijROZXUjG5vXa2OaCQO'\n",
    "domain = 'https://api.adsabs.harvard.edu/v1/'\n",
    "def resolve(references):\n",
    "    payload = {'parsed_reference': references}\n",
    "    response = requests.post(\n",
    "        url = domain + 'reference/xml',\n",
    "        headers = {'Authorization': 'Bearer ' + token,\n",
    "                 'Content-Type': 'application/json',\n",
    "                 'Accept':'application/json'},\n",
    "        data = json.dumps(payload))\n",
    "    if response.status_code == 200:\n",
    "        return json.loads(response.content)['resolved'], 200\n",
    "    else:\n",
    "        print('From reference status_code is', response.status_code)\n",
    "        return None, response.status_code\n",
    "\n",
    "# Resolve my references, results in 'total results' list\n",
    "references = [json.loads(ref) for ref in ref_list2]\n",
    "total_results = []\n",
    "print('Querying %d references with the Reference Service ...'%len(references))\n",
    "for i in range(0, len(references), 16):\n",
    "    results, status = resolve(references[i:i+16])\n",
    "    if results:\n",
    "        total_results += results\n",
    "#     else:\n",
    "#         break\n",
    "\n",
    "# Save the results to excel\n",
    "dt = pd.DataFrame(total_results)\n",
    "dt.to_excel(\"ref_review.xlsx\", index=False)\n",
    "print(\"Saved ref results to ref_review.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a862af3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
