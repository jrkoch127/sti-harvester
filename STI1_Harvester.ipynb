{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2761fe8",
   "metadata": {},
   "source": [
    "# STI/NTRS/PubSpace Harvester\n",
    "\n",
    "This notebook sends a query to STI/NTRS API for harvesting bibliographic records. Data returned are processed through the ADS Reference Service to weed out records already existing in ADS. The Excel workbook output can be reviewed and curated for ingest to ADS.\n",
    "\n",
    "#### STI Harvest\n",
    "1. Set name for output file (date + category) and filepath\n",
    "2. Select API endpoint for querying (regular STI Collection vs Pubspace) and parameters\n",
    "3. Connect to STI API and pull/store data locally\n",
    "\n",
    "#### ADS Bibcode Matching\n",
    "4. Tranform STI results into ref strings\n",
    "5. Query the ADS Reference Service API with ref strings, return bibcode matches\n",
    "6. Output new STI records (unmatched by RefService) for ingest review\n",
    "___\n",
    "NOTEBOOK OUTPUT: \n",
    "- Excel workbook: \"{name}_STIreview.xlsx\"\n",
    "   - Sheet 1: STI Harvest results\n",
    "   - Sheet 2: Reference results\n",
    "   - Sheet 3: New/unmatched items for ingest review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50c6476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import math\n",
    "import unicodedata\n",
    "from pyingest.serializers.classic import Tagged\n",
    "import re\n",
    "import csv\n",
    "\n",
    "# -- Set name of output file (date_category)\n",
    "name = \"2306Geo\"\n",
    "\n",
    "# -- Set local filepath to save output files\n",
    "filepath = \"/Users/sao/Documents/Python-Projects/STI/\"\n",
    "\n",
    "# -- Choose collection to query\n",
    "# path = \"/citations/search\"  # STI/NTRS Collection\n",
    "path = \"/pubspace/search\"     # Pubspace Collection\n",
    "\n",
    "# -- Input query parameters\n",
    "params = {\n",
    "         \"published\": {\"gte\":\"2020-01-01\"},\n",
    "         \"subjectCategory\": [\"Geophysics\"],\n",
    "#          \"stiType\": [\"ACCEPTED_MANUSCRIPT\"],\n",
    "         \"sort\": {\n",
    "             \"field\": \"id\",\n",
    "             \"order\": \"asc\"},\n",
    "         \"page\": {\"size\":100}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d585b1e1",
   "metadata": {},
   "source": [
    "## NTRS API Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b9f30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- API Setup\n",
    "base_url = \"https://ntrs.nasa.gov/api\"\n",
    "api_url = base_url + path\n",
    "MAX_RECORDS = 100\n",
    "\n",
    "def get_batch(api_url, params):\n",
    "    get_header = {'Accept': 'text/plain',\n",
    "                  'Content-type': 'application/json'}\n",
    "    buff = requests.post(api_url, headers=get_header, data=json.dumps(params)).json()\n",
    "    return buff\n",
    "\n",
    "def get_records(url, params):\n",
    "    records = []\n",
    "    \n",
    "    # Do the first query\n",
    "    try:\n",
    "        batch = get_batch(url, params)\n",
    "    except Exception as err:\n",
    "        raise Exception(\"Request to STI blew up: %s\" % err)\n",
    "        \n",
    "    # Count of total records\n",
    "    totrecs = batch['stats']['total']\n",
    " \n",
    "    # Store the first batch of records  \n",
    "    records += batch['results']\n",
    "    \n",
    "    # Print count of total records and pages\n",
    "    num_paginates = int(math.ceil((totrecs) / (1.0*MAX_RECORDS)))\n",
    "    print(\"Total records: %d \\nTotal pages: %d\" % (totrecs, num_paginates))\n",
    "          \n",
    "    # Continue requests\n",
    "    offset = MAX_RECORDS\n",
    "    for i in range(num_paginates):\n",
    "        params['page']['from'] = offset\n",
    "        try:\n",
    "            batch = get_batch(url, params)\n",
    "        except Exception as err:\n",
    "            raise URLError(\"Request to STI blew up: %s\" % err)\n",
    "        records += batch['results']\n",
    "        offset += MAX_RECORDS\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e7818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Run API Request\n",
    "from_sti = get_records(api_url, params)\n",
    "\n",
    "# Normalize json results to generate DataFrame\n",
    "dt = pd.json_normalize(from_sti, meta=['title'])\n",
    "\n",
    "# List of desired fields\n",
    "desired_fields = [\n",
    "    \"id\",\n",
    "    \"subjectCategories\",\n",
    "    \"fundingNumbers\",\n",
    "    \"authorAffiliations\",\n",
    "    \"title\",\n",
    "    \"stiType\",\n",
    "    \"abstract\",\n",
    "    \"publications\",\n",
    "    \"center.code\",\n",
    "    \"otherReportNumbers\",\n",
    "    \"keywords\",\n",
    "    \"sourceIdentifiers\",\n",
    "    \"meetings\"\n",
    "]\n",
    "\n",
    "# Grab desired fields if present in dt\n",
    "df = dt[[col for col in desired_fields if col in dt.columns]]\n",
    "\n",
    "# Drop rows where Document ID is null\n",
    "df = df.dropna(subset=['id'])\n",
    "\n",
    "# Drop duplicates by Document ID\n",
    "df = df.drop_duplicates(subset=['id'], keep='last')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ed8e32",
   "metadata": {},
   "source": [
    "## Metadata Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f942f6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TITLE/T\n",
    "lsT = [t if t else '' for t in df['title']]\n",
    "\n",
    "# ABSTRACT/B\n",
    "lsB = [b if b else '' for b in df['abstract']]\n",
    "\n",
    "# KEYWORDS/K\n",
    "lsK = [', '.join(k) if isinstance(k, list) else '' for k in df['keywords']]\n",
    "\n",
    "# STI Subject Categories - If any of the ES subject categories, insert 'Earth Science'\n",
    "EScats = ['Geosciences', \n",
    "          'Earth Resources and Remote Sensing', \n",
    "          'Energy Production and Conversion',\n",
    "          'Environment Pollution',\n",
    "          'Geophysics', \n",
    "          'Meteorology And Climatology',\n",
    "          'Oceanography']\n",
    "STIsubcats = ['; '.join(s + ['Earth Science']) if any(cat in s for cat in EScats) else s for s in df['subjectCategories']]\n",
    "\n",
    "\n",
    "# COLLECTION/W\n",
    "lsW = []\n",
    "for s in STIsubcats:\n",
    "    if \"Astronomy\" in s:\n",
    "        lsW.append('AST')\n",
    "    elif \"Geophysics\" in s:\n",
    "        lsW.append('')\n",
    "    elif \"Physics\" in s:\n",
    "        lsW.append('PHY')\n",
    "    else:\n",
    "        lsW.append('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b237ab3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTHORS/A & AFFILIATIONS/F\n",
    "\n",
    "# Function to reformat author names if not already in format \"Last, First\"\n",
    "    # Also appends periods '.' to initials if necessary\n",
    "def reformat_author(author_name):\n",
    "   \n",
    "    # If comma in author name, assume it's in \"Last, First\" format already; no need to reformat\n",
    "    if ', ' in author_name:\n",
    "        name_parts = author_name.split(', ')\n",
    "        last_name = name_parts[0]\n",
    "        \n",
    "        if len(name_parts) > 1:\n",
    "            first_middle_parts = name_parts[1].split()\n",
    "            first_name = first_middle_parts[0] if first_middle_parts else ''\n",
    "            middle_name = ' '.join(first_middle_parts[1:]) if len(first_middle_parts) > 1 else ''\n",
    "        else:\n",
    "            first_name = ''\n",
    "            middle_name = ''\n",
    "\n",
    "        # Check and add period to first name if necessary\n",
    "        if len(first_name) == 1 and not first_name.endswith('.'):\n",
    "            first_name += '.'\n",
    "\n",
    "        # Check and add period to middle name if necessary\n",
    "        initials = middle_name.split()\n",
    "        for i in range(len(initials)):\n",
    "            if len(initials[i]) == 1 and not initials[i].endswith('.'):\n",
    "                initials[i] += '.'\n",
    "        middle_name = ' '.join(initials)\n",
    "\n",
    "        formatted_name = \"{}, {}\".format(last_name, first_name)\n",
    "        if middle_name:\n",
    "            formatted_name += \" {}\".format(middle_name)\n",
    "\n",
    "        return formatted_name\n",
    "    \n",
    "    # Reformat if there's no comma in author_name\n",
    "    else:\n",
    "        name_list = author_name.split()\n",
    "        if len(name_list) > 0:\n",
    "            first_name = name_list[0]\n",
    "            last_name = \"\"\n",
    "            middle_name = \"\"\n",
    "            suffix = \"\"\n",
    "\n",
    "            # Check for suffixes\n",
    "            suffixes = [\"Jr.\", \"Sr.\", \"II\", \"III\"]\n",
    "            if len(name_list) > 1:\n",
    "                last_word = name_list[-1]\n",
    "                if last_word in suffixes:\n",
    "                    suffix = last_word\n",
    "                    name_list = name_list[:-1]\n",
    "\n",
    "                # Format name parts\n",
    "                if len(name_list) > 1:\n",
    "                    last_name = name_list[-1]\n",
    "                    middle_name = \" \".join(name_list[1:-1])\n",
    "                else:\n",
    "                    last_name = name_list[0]\n",
    "\n",
    "                # Check and add period to first name if necessary\n",
    "                if len(first_name) == 1 and not first_name.endswith('.'):\n",
    "                    first_name += '.'\n",
    "\n",
    "                # Check and add period to middle name if necessary\n",
    "                if len(middle_name) == 1 and not middle_name.endswith('.'):\n",
    "                    middle_name += '.'\n",
    "\n",
    "            # Construct formatted name\n",
    "            formatted_name = \"{}, {}\".format(last_name, first_name)\n",
    "            if middle_name:\n",
    "                formatted_name += \" {}\".format(middle_name)\n",
    "            if suffix:\n",
    "                formatted_name += \", {}\".format(suffix)\n",
    "            return formatted_name\n",
    "\n",
    "\n",
    "# Grab Author/Affiliation Metadata\n",
    "lsA = [] # Authors List\n",
    "lsF = [] # Affiliations List\n",
    "\n",
    "for authors in df['authorAffiliations']:\n",
    "    row_authors = []\n",
    "    row_affils = []\n",
    "\n",
    "    for entry in authors:\n",
    "        author = entry['meta']['author'].get('name', '')      # Concat author names\n",
    "        reformatted = reformat_author(author)\n",
    "        row_authors.append(reformatted)\n",
    "\n",
    "        organization = entry['meta'].get('organization', {})  # Concat aff names\n",
    "        aff_name = organization.get('name', '')\n",
    "        aff_loc = organization.get('location', '')            # Concat aff locations\n",
    "        affil = ', '.join(filter(None, [aff_name, aff_loc]))\n",
    "\n",
    "        orcid = entry['meta']['author'].get('orcidId', '')    # Concat orcids\n",
    "        if orcid:\n",
    "            affil += ' <ID system=\"ORCID\">{}</ID>'.format(orcid)\n",
    "\n",
    "        row_affils.append(affil)\n",
    "\n",
    "    lsA.append(row_authors)\n",
    "    lsF.append(row_affils)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da566c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STI Stuff\n",
    "STI_ids = [d if d else '' for d in df['id']]\n",
    "STI_types = [t if t else '' for t in df['stiType']]\n",
    "centers = [c if c else '' for c in df['center.code']]\n",
    "    \n",
    "# Report Numbers\n",
    "reportNums = []\n",
    "for numbers in df['otherReportNumbers']:\n",
    "    if isinstance(numbers, list):\n",
    "        for entry in numbers:\n",
    "            if numbers != '[]':\n",
    "                reportNums.append(numbers)\n",
    "    else:\n",
    "        reportNums.append('')\n",
    "    \n",
    "# Funding Numbers\n",
    "fundingNums = []\n",
    "for numbers in df['fundingNumbers']:\n",
    "    if isinstance(numbers, list):\n",
    "        for entry in numbers:\n",
    "            if isinstance(entry, dict) and 'type' in entry and 'number' in entry:\n",
    "                fundType = entry['type']\n",
    "                fundNum = entry['number']\n",
    "                fundingNums.append('{}: {}'.format(fundType, fundNum))\n",
    "            else:\n",
    "                fundingNums.append('')\n",
    "\n",
    "# Concat Numbers                \n",
    "otherNums = []            \n",
    "for STIid, reportNum, fundNum in zip(STI_ids, reportNums, fundingNums):\n",
    "    others = ''\n",
    "    if STIid:\n",
    "        sti = \"STI: {}\".format(STIid)\n",
    "        others += sti\n",
    "    if reportNum:\n",
    "        others += \", \" + \", \".join(r for r in reportNum)\n",
    "    if fundNum:\n",
    "        others += \", \" + fundNum\n",
    "    otherNums.append(others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee1884b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PUBDATE/D, PUBLICATION/J, LINK PROPERTIES/I \n",
    "## -- future work -> Insert meeting info; sub a different date for missing pubdate\n",
    "\n",
    "# Metadata for refstrings\n",
    "pubs_ls = [] # Pubnames\n",
    "vols_ls = [] # Volumes\n",
    "dois_ls = [] # DOIs\n",
    "\n",
    "# Metadata for data curation/ingest\n",
    "lsD = []  # Pubdates\n",
    "lsJ = []  # Journal/Pub\n",
    "lsI = []  # Properties/Links\n",
    "\n",
    "for pubs, meets, idents, docID in zip(df['publications'], df['meetings'], df['sourceIdentifiers'], STI_ids):\n",
    "    j = ''\n",
    "    links = ''\n",
    "    pubnames = ''\n",
    "    volumes = ''\n",
    "    dois = ''\n",
    "    \n",
    "    for entry in pubs:\n",
    "        pubdate = entry.get('publicationDate', '')[:10]\n",
    "        publication_name = entry.get('publicationName', '')\n",
    "        vol = entry.get('volume', '')\n",
    "        issue = entry.get('issue', '')\n",
    "        publisher = entry.get('publisher', '')\n",
    "        issn = entry.get('issn', '')\n",
    "        eissn = entry.get('eissn', '')\n",
    "        isbn = entry.get('isbn', '')\n",
    "        eisbn = entry.get('eisbn', '')\n",
    "        url = entry.get('url', '')\n",
    "        doi = entry.get('doi', '')\n",
    "            \n",
    "        if publication_name:\n",
    "            j += publication_name.rstrip(\" \")\n",
    "            pubnames += publication_name.rstrip(\" \")\n",
    "        if vol:\n",
    "            j += ', Vol. {}'.format(vol)\n",
    "            volumes += vol\n",
    "        if issue:\n",
    "            j += ', Issue {}'.format(issue)\n",
    "        if publisher:\n",
    "            j += ', Published by {}, {}'.format(publisher, pubdate[:4]).rstrip(\" \").lstrip(\" \")\n",
    "        if issn:\n",
    "            j += ', ISSN: {}'.format(issn)\n",
    "        if eissn:\n",
    "            j += ', eISSN: {}'.format(eissn)\n",
    "        if isbn:\n",
    "            j += ', ISBN: {}'.format(isbn)\n",
    "        if eisbn:\n",
    "            j += ', eISBN: {}'.format(isbn)\n",
    "        j = j.lstrip(', ')\n",
    "\n",
    "        # Concat link info from publications field\n",
    "        doi = doi.replace('https://', '').replace('doi:', '').replace('doi.org/', '').replace('DOI:', '').lstrip(\" \")\n",
    "        if url and doi:\n",
    "            links += 'ELECTR: {}; DOI: {}'.format(url, doi)\n",
    "            dois += doi\n",
    "        elif url:\n",
    "            links += 'ELECTR: {}'.format(url)\n",
    "        elif doi:\n",
    "            links += 'DOI: {}'.format(doi)\n",
    "            dois += doi\n",
    "    \n",
    "    if isinstance(meets, list):         # Concat meeting info\n",
    "        for entry in meets:\n",
    "            if isinstance(entry, dict) and 'name' in entry and entry.get('name') != '':\n",
    "                if 'location' in entry and entry.get('location') != '':\n",
    "                    meet = entry['name']\n",
    "                    meet_loc = entry['location']\n",
    "                    j += \"; {}, {}\".format(meet, meet_loc)\n",
    "                else:\n",
    "                    j += \"; {}.\".format(meet)\n",
    "    \n",
    "    if isinstance(idents, list):       # Concat additional links from sourceIdentifiers field\n",
    "        for entry in idents:\n",
    "            if isinstance(entry, dict):\n",
    "                if entry.get('type') == 'URL' and 'number' in entry:\n",
    "                    if 'arXiv' in entry['number'] or 'arxiv.org' in entry['number']:\n",
    "                        arxiv = entry['number'].replace('arXiv:','')\n",
    "                        links += '; ARXIV: {}'.format(arxiv)\n",
    "                    else:\n",
    "                        url = entry['number']\n",
    "                        links += '; ELECTR: {}'.format(url)\n",
    "                if entry.get('type') == 'DOI' and 'number' in entry:\n",
    "                    doi = entry['number'].replace('doi:', '')\n",
    "                    links += '; DOI: {}'.format(doi)\n",
    "                    dois += doi\n",
    "    \n",
    "    \n",
    "    links += '; ELECTR: https://ntrs.nasa.gov/citations/{}'.format(docID)\n",
    "        \n",
    "    # Append metadata to lists for refstrings and curation/ingest\n",
    "    pubs_ls.append(pubnames)\n",
    "    vols_ls.append(volumes)\n",
    "    dois_ls.append(dois)\n",
    "    lsD.append(pubdate)\n",
    "    lsJ.append(j.lstrip('; '))\n",
    "    lsI.append(links.lstrip('; '))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f0fc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip metadata into list of records for curation/ingest\n",
    "records = []\n",
    "for A, F, T, D, J, B, I, W, K, subcat, STI_type, center, other in zip(lsA, lsF, lsT, lsD, lsJ, lsB, lsI, lsW, lsK, STIsubcats, STI_types, centers, otherNums):\n",
    "    record = {\n",
    "        \"authors\": '; '.join([a for a in A if a is not None]),\n",
    "        \"affiliations\": '; '.join([f for f in F if f is not None]),\n",
    "        \"title\": T,\n",
    "        \"pubdate\": D,\n",
    "        \"publication\": J,\n",
    "        \"abstract\": B,\n",
    "        \"properties\": I,\n",
    "        \"collection\": W,\n",
    "        \"keywords\": K,\n",
    "        \"STI subject categories\": subcat,\n",
    "        \"type\": STI_type,\n",
    "        \"NASA center\": center,\n",
    "        \"other\": other\n",
    "    }\n",
    "    records.append(record)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88322b3",
   "metadata": {},
   "source": [
    "## Reference Resolver Service - Match Existing ADS Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1b4804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare reference strings to query ADS Reference Resolver Service\n",
    "list_for_REFS = []\n",
    "\n",
    "    # -- Option to generate refstrings of {Author, Year, Publication, Volume}\n",
    "for A, D, T, pub, vol, doi in zip(lsA, lsD, lsT, pubs_ls, vols_ls, dois_ls):\n",
    "    \n",
    "    # Year = first 4 digits of pubdate\n",
    "    D = D[:4]\n",
    "    \n",
    "    # Grab just first author\n",
    "    if isinstance(A, list):\n",
    "        A = '; '.join(A[:10])\n",
    "    \n",
    "    # Concat refstrings\n",
    "    if A and D and pub and vol:\n",
    "        ref = {\n",
    "            \"refstr\":\"%s, %s, %s %s, %s\"%(A, D, pub, vol, doi), \n",
    "            \"authors\": A, \n",
    "            \"year\": D, \n",
    "            \"journal\":\"%s %s\"%(pub, vol),\n",
    "            \"doi\": doi\n",
    "        }\n",
    "\n",
    "    elif A and D and T:\n",
    "        ref = {\n",
    "            \"refstr\":\"%s, %s, %s\"%(A, D, T), \n",
    "            \"authors\":\"%s\"%A, \n",
    "            \"year\":\"%s\"%D, \n",
    "            \"title\": \"%s\"%T\n",
    "        }\n",
    "    \n",
    "    ref_string = json.dumps(ref, ensure_ascii=False)\n",
    "    ref_string = ref_string.replace(\"\\\\t\", \"\")\n",
    "\n",
    "    list_for_REFS.append(ref_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894ceb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Reference Resolver Service Setup\n",
    "\n",
    "# ADS Prod API Token\n",
    "token = 'pHazHxvHjPVPAcotvj7DIijROZXUjG5vXa2OaCQO'\n",
    "domain = 'https://api.adsabs.harvard.edu/v1/'\n",
    "\n",
    "# Reference Service API request, querying my 'references' list\n",
    "def resolve(references):\n",
    "    payload = {'parsed_reference': references}\n",
    "    response = requests.post(\n",
    "        url = domain + 'reference/xml',\n",
    "        headers = {'Authorization': 'Bearer ' + token,\n",
    "                 'Content-Type': 'application/json',\n",
    "                 'Accept':'application/json'},\n",
    "        data = json.dumps(payload))\n",
    "    if response.status_code == 200:\n",
    "        return json.loads(response.content)['resolved'], 200\n",
    "    else:\n",
    "        print('From reference status_code is ', response.status_code)\n",
    "    return None, response.status_code\n",
    "\n",
    "# -- Run Reference Resolver Service\n",
    "references = list_for_REFS\n",
    "references = [ref.replace(\"\\n\",\" \") for ref in references]\n",
    "references = [json.loads(ref) for ref in references]\n",
    "\n",
    "# Resolve my references, results in 'total results' list\n",
    "total_results = []\n",
    "print(\"Querying %d references with the Reference Service ...\"%len(references))\n",
    "for i in range(0, len(references), 16):\n",
    "    results, status = resolve(references[i:i+16])\n",
    "    if results:\n",
    "        total_results += results\n",
    "\n",
    "# REF RESULTS (df2): Format all results to tab separated list\n",
    "bibcodes = []\n",
    "bibcode_counter = 0\n",
    "no_match_counter = 0\n",
    "for r in total_results:   # if match found, return refstring, bibcode, and score\n",
    "    if r['bibcode']!='...................':\n",
    "        bibcodes.append(r['refstring'] + \"\\t\" + r['bibcode'] + \"\\t\" + r['score'])\n",
    "        bibcode_counter += 1\n",
    "    else:                # if no match found, return refstring and error comment\n",
    "        bibcodes.append(r['refstring'] + \"\\t\\t\\t\" + r.get('comment', ''))\n",
    "        no_match_counter += 1\n",
    "        \n",
    "df2 = pd.DataFrame(line.split(\"\\t\") for line in bibcodes if line)   # Generate data frame\n",
    "df2.columns = ['refstring','bibcode','score','comment']\n",
    "df2 = df2.drop_duplicates(subset='refstring')                       # Drop duplicates\n",
    "df2 = df2.sort_values(by=['score','comment'],ascending=False)       # sort by score, then comment\n",
    "\n",
    "# NEW INGESTS (df3): Format non-matched results to new list for ingest\n",
    "to_ingest = []\n",
    "for record, result in zip(records, total_results):\n",
    "    if result['bibcode'] == '...................':\n",
    "        to_ingest.append(record)\n",
    "\n",
    "df3 = pd.json_normalize(to_ingest)                  # Generate data frame of new ingests\n",
    "df3 = df3.sort_values(by=['title'])                 # sort by title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9b1746",
   "metadata": {},
   "source": [
    "## Output Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a334df25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to an excel file with multiple sheets\n",
    "outfile = name + \"_STIreview.xlsx\"\n",
    "with pd.ExcelWriter(filepath + outfile) as writer:\n",
    "    df.to_excel(writer, sheet_name='sti_output', index=False)   # Original NTRS API output\n",
    "    df2.to_excel(writer, sheet_name='ref_results', index=False) # Reference Resolver results\n",
    "    df3.to_excel(writer, sheet_name='ingest_new', index=False)  # non-matched records for ingest review\n",
    "    \n",
    "# Print summary\n",
    "print(\n",
    "    'RESULTS SUMMARY\\n\\n STI API Query Parameters: {}\\n'.format(params),\n",
    "    '\\n > Records generated: {}\\n > Records matched (ADS): {}\\n > Records for ingest: {}'.format(len(references), bibcode_counter, len(to_ingest)),\n",
    "    '\\n Results saved to {}'.format(outfile)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69712fb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0f1205",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
